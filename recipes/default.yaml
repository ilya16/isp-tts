version: ${version:}

_general_:
  device: cuda:0
  seed: 23

  _dirname_: results
  _label_: ???

  output_dir:  # either a full path or a list of path elements
    - ${_general_._dirname_}
#    - ${version}
    - ${model._name_}
#    - ${model._version_}
    - ${date:}
    - ${_general_._label_}

  resume_from_checkpoint:
  warm_start: false
  finetune_layers: []


dataset:
  _name_: ???

collator:
  _name_: ???


model:
  _name_: ???
  _version_: v0.1


criterion:
  _name_: ???


evaluator:
  _name_: ???


trainer:
  # general
  output_dir: ${_general_.output_dir}

  do_train: true
  do_eval: true
  eval_mode: false

  device: ${_general_.device}
  seed: ${_general_.seed}

  # accelerator
  accelerator:
    mixed_precision: fp16
    log_with: [tensorboard]  # `tensorboard`, `wandb`

  # logging
  log_dir: logs
  log_to_file: true

  project_name: ${model._name_}
  tracker_kwargs:
    tensorboard: {}
    wandb:
      name: ${version:}/${date:}/${_general_._label_}

  log_strategy: steps
  log_steps: 5
  log_first_step: true
  log_raw_to_console: true

  disable_tqdm: false
  progress_steps: 5
  progress_metrics: ["loss"]

  # data
  num_workers: 4
  pin_memory: true
  shuffle: true   # will shuffle training set only

  # training & evaluation
  epochs: 100
  max_steps: -1
  batch_size: 32

  eval_batch_size: 64
  eval_batches:

  eval_strategy: epoch    # evaluation and checkpoint steps measure: `epoch` or `steps`
  eval_steps: 1   # how frequently (in epochs/steps) model evaluation should be performed

  optimization:
    optimizer:
      _target_: adamw
      lr: !!float 2e-4
      weight_decay: !!float 1e-2
    lr_scheduler:
      _target_: exponential
      gamma: 0.995
    grad_clip: 1.0
    grad_accum_steps: 1

  # checkpointing
  save_strategy: ${trainer.eval_strategy}
  save_steps: ${trainer.eval_steps}
  save_optimizer: false
  save_best_only: true
  save_rewrite_checkpoint: false

  metric_for_best_model: loss
  metric_maximize: false

  resume_from_checkpoint: ${_general_.resume_from_checkpoint}
  warm_start: ${_general_.warm_start}
  ignore_layers: []
  ignore_mismatched_keys: true
  finetune_layers: ${_general_.finetune_layers}
  restore_lr: true

  callbacks:
    epoch_seed:
      _target_: epoch_seed